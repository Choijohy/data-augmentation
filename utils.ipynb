{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face의 트랜스포머 모델을 설치\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzic3lUUqFdR",
        "outputId": "eed80359-9faa-402d-85e0-5d906f59ef54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM,\n",
        "    AdamW,\n",
        "    get_scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "wqaRMZJ8p01N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_tokens(tokenizer, input_ids:torch.Tensor, mlm_prob:float=0.15, do_rep_random:bool=True):\n",
        "    '''\n",
        "        Copied from huggingface/transformers/data/data_collator - torch.mask_tokens()\n",
        "        Prepare masked tokens inputs/labels for masked language modeling\n",
        "        if do_rep_random is True:\n",
        "            80% MASK, 10% random, 10% original\n",
        "        else:\n",
        "            100% MASK\n",
        "    '''\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    probability_matrix = torch.full(labels.shape, mlm_prob)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value = 0.0)\n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100 # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    mask_rep_prob = 0.8\n",
        "    if not do_rep_random:\n",
        "        mask_rep_prob = 1.0\n",
        "    \n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, mask_rep_prob)).bool() & masked_indices\n",
        "    input_ids[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    if do_rep_random:\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "        input_ids[indices_random] = random_words[indices_random]\n",
        "\n",
        "    return input_ids, labels"
      ],
      "metadata": {
        "id": "Z2OW-v-eqTUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenizer(args):\n",
        "    if args.do_pred:\n",
        "        tokenizer_path = args.tuned_model_path\n",
        "    else:\n",
        "        tokenizer_path = args.model_name_or_path\n",
        "\n",
        "    return AutoTokenizer.from_pretrained(tokenizer_path)"
      ],
      "metadata": {
        "id": "RcyExX_zr4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlMN1aESpwEc"
      },
      "outputs": [],
      "source": [
        "def initialize_model(args, total_steps):\n",
        "    model = AutoModelForMaskedLM.from_pretrained(args.model_name_or_path)\n",
        "\n",
        "    if (torch.cuda.is_available()) and (not args.no_cuda):\n",
        "        if (not args.multi):\n",
        "            device = \"cuda:\" + str(args.dev_num)\n",
        "        else:\n",
        "            n_dev = torch.cuda.device_count()\n",
        "            dev_list = list(range(n_dev))\n",
        "            model = nn.DataParallel(model, device_ids = dev_list, output_device=dev_list[0])\n",
        "            device = dev_list[0]\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = args.learning_Rate,\n",
        "                    eps = args.eps,\n",
        "                    weight_decay = args.weight_decay)\n",
        "    \n",
        "    scheduler = get_scheduler(args.scheduler_name,\n",
        "                            optimizer,\n",
        "                            num_warmup_steps = int(total_steps * args.warmup_proportion),\n",
        "                            num_training_steps = total_steps)\n",
        "    \n",
        "    return model, optimizer, scheduler, device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model_with_ds(args):\n",
        "    import deepspeed\n",
        "\n",
        "    model = AutoModelForMaskedLM.from_pretrained(args.model_name_or_path)\n",
        "    model, optimizer, _, scheduler = deepspeed.initialize(model=model, args=args, model_parameters=model.parameters())\n",
        "\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "TtwxvNo_sWSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}